## Deep Dive Execution – Architecture & Plan

### Goal
Let users run follow-up commands (generated by the `deep-dive` plugin) on demand, capture the output/artifacts, and view the results alongside the initial scan.

### Requirements
1. **Trigger model**
   - Each scan result already contains a list of deep-dive `commands`.
   - Users need a way to select which command to run (individual or all) from the UI.

2. **Execution environment**
   - Commands must run asynchronously on the backend.
   - Output (stdout/stderr) and generated files should be captured.
   - Provide a sandbox/agent mechanism so dangerous commands don’t run unchecked (initial version can run locally with a configurable allowlist).

3. **Persistence**
   - Store task metadata (command, target, status, output paths) in DB.
   - Artifacts go into the existing `web_runs/<job_id>/deepdive/<task_id>/` directory.

4. **Status tracking**
   - Expose REST endpoints for creating tasks, polling status, fetching output.
   - Include task summaries in the existing `/ws/status` payload (future iteration).

5. **UI workflow**
   - In `ScanDetail`, list deep-dive commands with action buttons (`Run`, `View output`).
   - Show task queue with status badges, logs, and download links.

### Current Status
- Deep-dive controls ship in the web UI: commands are grouped by service/port with individual **Run** buttons and a global **Run All** action.
- Live task updates flow through the `/ws/status` stream; the execution queue table lists return codes and timestamps.
- Stdout/stderr can be expanded inline once tasks finish, and a manual refresh is available if the websocket connection drops.
- Backend execution is protected by an allowlist (configurable via env/file); the UI highlights blocked commands so operators know what will run.

### Using the Web UI
1. Launch a scan that enables the `deep-dive` plugin (from the CLI or **New Scan** form).
2. Select the job in **Scan Details** and open the **Deep-Dive Follow-ups** section.
3. Choose **Run** next to a specific command or **Run All** to enqueue every suggestion.
4. Monitor the **Execution Queue** table for status changes and open the output viewer to read captured logs.

### Architecture
1. **Backend**
   - New SQLModel table: `DeepDiveTaskRecord` with fields:
     - `id`, `scan_job_id`, `command`, `status` (`pending`, `running`, `completed`, `failed`), `created_at`, `updated_at`, `stdout_path`, `stderr_path`, `return_code`.
   - Background execution via `asyncio.to_thread` or a Celery worker (phase 1: local thread).
   - API endpoints:
     - `POST /scans/{job_id}/deepdive`: create tasks (one or all commands).
     - `GET /scans/{job_id}/deepdive`: list tasks for a job.
     - `GET /deepdive/{task_id}`: fetch details/output.
   - Extend `JobManager` or add `DeepDiveManager` to handle execution.

2. **Frontend**
   - Update `ScanDetail` to render a table of commands with action buttons.
   - Modal or drawer to display captured stdout/stderr.
   - Use toast notifications / status chips for progress.

### Incremental Deliverables
1. **MVP (Local Execution)**
   - DB schema, backend APIs, basic executor, UI list + run button.
   - Output capture stored in files, displayed via API.
2. **Enhancements**
   - Worker dispatch (choose which agent runs the command).
   - Allowlist/safety checks; configure per command family.
   - WebSocket updates for task progress.
3. **Future**
   - Notification hooks (email/webhook on completion).
   - Editable command templates with custom arguments.

### Next Steps
1. Extend command controls with per-worker dispatch and sandbox toggles.
2. Add automated coverage (API + UI) around the deep-dive lifecycle.
3. Extend the UI to choose a remote worker once orchestrator integration lands.

### Testing
- Integration checks live under `webapp/tests/test_deepdive_api.py`. Run them with `venv/bin/python -m pytest webapp/tests/test_deepdive_api.py` (install `pytest`/`httpx` in the virtualenv if you haven’t already).
